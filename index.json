[{"content":"演示案例架构图 中间件服务构建 根据SpringCloudAlibaba与SpringCloud的对应关系,本次案例实现采用中间件/框架版本如下\nSpring Boot: 2.6.13 Spring Cloud: 2021.0.5 Spring Cloud Alibaba: 2021.0.5.0 Nacos: 2.2.0 Seata: 1.6.1 Mysql: 8.1.0 Nacos构建 从这里下载Nacos预编译包,nacos配置数据我们采用持久化到数据库的方式\n首先准备nacos的数据库实例:\nversion: \u0026#39;3.1\u0026#39; services: nacos: image: mysql:8.1.0 restart: always container_name: mysql_nacos environment: MYSQL_ROOT_PASSWORD: 5566 command: --default-authentication-plugin=mysql_native_password --character-set-server=utf8mb4 --collation-server=utf8mb4_general_ci --explicit_defaults_for_timestamp=true --lower_case_table_names=1 ports: - 3306:3306 volumes: - ./config/data/mysql:/var/lib/mysql 然后修改Nacos包中的scheme导入到该数据库实例中\n修改conf目录下的application.properties文件,主要修改以下几项(单机模式,集群参考官方文档修改):\n#*************** Config Module Related Configurations ***************# ### If use MySQL as datasource: spring.datasource.platform=mysql ### Count of DB: db.num=1 ### Connect URL of DB: db.url.0=jdbc:mysql://127.0.0.1:3306/nacos?characterEncoding=utf8\u0026amp;connectTimeout=1000\u0026amp;socketTimeout=3000\u0026amp;autoReconnect=true\u0026amp;useUnicode=true\u0026amp;useSSL=false\u0026amp;serverTimezone=UTC db.user.0=root db.password.0=5566 启动nacos:\n/Users/fanzhengxiang/data/service/nacos2.2.0/bin/startup.sh -m standalone\nSeata构建 从这里下载seata的预编译包,本次实验seata数据同样持久化到数据库中,准备一个数据库实例\nversion: \u0026#39;3.1\u0026#39; services: seata_db: image: mysql:8.1.0 restart: always container_name: mysql_seata environment: MYSQL_ROOT_PASSWORD: 5566 command: --default-authentication-plugin=mysql_native_password --character-set-server=utf8mb4 --collation-server=utf8mb4_general_ci --explicit_defaults_for_timestamp=true --lower_case_table_names=1 ports: - 3307:3306 volumes: - ./config/data/mysql:/var/lib/mysql 将mysql的建表数据导入到seata的数据库实例中\n修改seata的启动配置文件conf/application.yml,主要修改config项,registry项目,store项目,以下是实例\n注意:由于mysql的版本为8,所以驱动连接为com.mysql.cj.jdbc.Driver,并且需要下载mysql8的java驱动到seata的lib报下\n# Copyright 1999-2019 Seata.io Group. # # Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. server: port: 7091 spring: application: name: seata-server logging: config: classpath:logback-spring.xml file: path: ${user.home}/logs/seata # extend: # logstash-appender: # destination: 127.0.0.1:4560 # kafka-appender: # bootstrap-servers: 127.0.0.1:9092 # topic: logback_to_logstash console: user: username: seata password: seata seata: config: # support: nacos 、 consul 、 apollo 、 zk 、 etcd3 type: nacos nacos: server-addr: 127.0.0.1:8848 namespace: group: SEATA_GROUP username: password: context-path: ##if use MSE Nacos with auth, mutex with username/password attribute #access-key: #secret-key: data-id: seataServer.properties registry: # support: nacos 、 eureka 、 redis 、 zk 、 consul 、 etcd3 、 sofa type: nacos nacos: application: seata-server server-addr: 127.0.0.1:8848 group: SEATA_GROUP namespace: cluster: default username: password: context-path: ##if use MSE Nacos with auth, mutex with username/password attribute #access-key: #secret-key: store: db: datasource: druid db-type: mysql ## 这里注意mysql8的驱动连接为com.mysql.cj.jdbc.Driver driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://127.0.0.1:3307/seata?rewriteBatchedStatements=true user: root password: 5566 min-conn: 10 max-conn: 100 global-table: global_table branch-table: branch_table lock-table: lock_table distributed-lock-table: distributed_lock query-limit: 1000 max-wait: 5000 # server: # service-port: 8091 #If not configured, the default is \u0026#39;${server.port} + 1000\u0026#39; security: secretKey: SeataSecretKey0c382ef121d778043159209298fd40bf3850a017 tokenValidityInMilliseconds: 1800000 ignore: urls: /,/**/*.css,/**/*.js,/**/*.html,/**/*.map,/**/*.svg,/**/*.png,/**/*.ico,/console-fe/public/**,/api/v1/auth/login 启动seata: nohup sh bin/seata-server.sh -p 18091 -n 1 \u0026amp;\nnacos: 注册信息\nseata控制台\n代码准备 源码在这里查看\n准备三个微服务模块的数据库实例\nversion: \u0026#39;3.1\u0026#39; services: seata_order: image: mysql:8.1.0 restart: always container_name: seata_order environment: MYSQL_ROOT_PASSWORD: 5566 command: --default-authentication-plugin=mysql_native_password --character-set-server=utf8mb4 --collation-server=utf8mb4_general_ci --explicit_defaults_for_timestamp=true --lower_case_table_names=1 ports: - 3308:3306 volumes: - ./order/config/data/mysql:/var/lib/mysql seata_inventory: image: mysql:8.1.0 restart: always container_name: seata_inventory environment: MYSQL_ROOT_PASSWORD: 5566 command: --default-authentication-plugin=mysql_native_password --character-set-server=utf8mb4 --collation-server=utf8mb4_general_ci --explicit_defaults_for_timestamp=true --lower_case_table_names=1 ports: - 3309:3306 volumes: - ./inventory/config/data/mysql:/var/lib/mysql seata_account: image: mysql:8.1.0 restart: always container_name: seata_account environment: MYSQL_ROOT_PASSWORD: 5566 command: --default-authentication-plugin=mysql_native_password --character-set-server=utf8mb4 --collation-server=utf8mb4_general_ci --explicit_defaults_for_timestamp=true --lower_case_table_names=1 ports: - 3310:3306 volumes: - ./account/config/data/mysql:/var/lib/mysql 项目依赖:\n\u0026lt;properties\u0026gt; \u0026lt;maven.compiler.target\u0026gt;8\u0026lt;/maven.compiler.target\u0026gt; \u0026lt;maven.compiler.source\u0026gt;8\u0026lt;/maven.compiler.source\u0026gt; \u0026lt;spring.boot.version\u0026gt;2.6.13\u0026lt;/spring.boot.version\u0026gt; \u0026lt;spring.cloud.version\u0026gt;2021.0.5\u0026lt;/spring.cloud.version\u0026gt; \u0026lt;spring.cloud.alibaba.version\u0026gt;2021.0.5.0\u0026lt;/spring.cloud.alibaba.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spring.boot.version}\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-dependencies\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spring.cloud.version}\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-alibaba-dependencies\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spring.cloud.alibaba.version}\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.18.28\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt; 以订单服务为例\n项目配置文件application.yml\nserver: port: 8081 # 端口 spring: application: name: order-service datasource: url: jdbc:mysql://127.0.0.1:3308/seata_order?useSSL=false\u0026amp;useUnicode=true\u0026amp;characterEncoding=UTF-8 driver-class-name: com.mysql.cj.jdbc.Driver username: root password: 5566 cloud: nacos: discovery: server-addr: 127.0.0.1:8848 seata: application-id: ${spring.application.name} tx-service-group: ${spring.application.name}-group service: vgroup-mapping: order-service-group: default registry: type: nacos nacos: cluster: default server-addr: localhost namespace: 其中账户服务和库存服务的Feign接口如下\nimport com.aires.feign.dto.AccountReduceBalanceDTO; import org.springframework.cloud.openfeign.FeignClient; import org.springframework.web.bind.annotation.PostMapping; import org.springframework.web.bind.annotation.RequestBody; /** * `account-service` 服务的 Feign 客户端 */ @FeignClient(name = \u0026#34;account-service\u0026#34;) public interface AccountServiceFeignClient { @PostMapping(\u0026#34;/account/reduce-balance\u0026#34;) void reduceBalance(@RequestBody AccountReduceBalanceDTO accountReduceBalanceDTO); } import com.aires.feign.dto.ProductReduceStockDTO; import org.springframework.cloud.openfeign.FeignClient; import org.springframework.web.bind.annotation.PostMapping; import org.springframework.web.bind.annotation.RequestBody; /** * `product-service` 服务的 Feign 客户端 */ @FeignClient(name = \u0026#34;product-service\u0026#34;) public interface ProductServiceFeignClient { @PostMapping(\u0026#34;/product/reduce-stock\u0026#34;) void reduceStock(@RequestBody ProductReduceStockDTO productReduceStockDTO); } 下单的核心业务代码\n@Service public class OrderServiceImpl implements OrderService { private Logger logger = LoggerFactory.getLogger(getClass()); @Resource private OrderDao orderDao; @Resource private AccountServiceFeignClient accountService; @Resource private ProductServiceFeignClient productService; @Override @GlobalTransactional public Integer createOrder(Long userId, Long productId, Integer price) { Integer amount = 1; // 购买数量，暂时设置为 1。 logger.info(\u0026#34;[createOrder] 当前 XID: {}\u0026#34;, RootContext.getXID()); // 扣减库存 productService.reduceStock(new ProductReduceStockDTO().setProductId(productId).setAmount(amount)); // 扣减余额 accountService.reduceBalance(new AccountReduceBalanceDTO().setUserId(userId).setPrice(price)); // 保存订单 OrderDO order = new OrderDO().setUserId(userId).setProductId(productId).setPayAmount(amount * price); orderDao.saveOrder(order); logger.info(\u0026#34;[createOrder] 保存订单: {}\u0026#34;, order.getId()); // 返回订单编号 return order.getId(); } } 分布式事务演示 // TODO\n","permalink":"https://www.atomicbot.cloud/posts/tech/seata_at%E6%A8%A1%E5%BC%8F%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E5%AE%9E%E8%B7%B5/","summary":"演示案例架构图 中间件服务构建 根据SpringCloudAlibaba与SpringCloud的对应关系,本次案例实现采用中间件/框架版本如下 Spring Boot: 2.6.13 Spring Cloud: 2021.0.5 Spring Cloud Alibaba: 2021.0.5.0 Nacos: 2.2.0 Seata: 1.6.1 Mysql: 8.1.0 Nacos构建 从这里下载Nacos预编译包,nacos配置数据我们采用持久化到数据库的方式 首先准备nacos","title":"Spring Cloud Feign \u0026\u0026 Seata At模式分布式事务实践"},{"content":"Spring Cloud Feign 在异步接口执行的过程中传递请求头Token 业务流程 用户发起请求后,在A服务中执行业务逻辑,执行过程中需要调用其他微服务B进行流程审批,简单方式是在A服务中所有逻辑(1,2,3)采用同步逻辑放在一个事务中,但是如果调用B服务的过程中,处理逻辑很复杂,会导致A服务的请求一直阻塞,用户体验很不好\n优化后的逻辑为将第2步,调整为异步逻辑,异步调用服务B,可以使A服务快速完成逻辑处理,相应用户,B服务在流程发起成功之后,再回调A服务,回写流程发起的结果(流程发起成功或者流程发起失败的原因),此时就引申出一个问题: 在异步调用时,feign接口的请求会丢失掉请求头中的Token,导致在调用服务B时出现401\n解决方案 网上大多数的解决办法都是抄来抄去使用拦截器方式实现,不推荐,我这里采用的是Feign官方的解决方案,Feign Builder\n手动创建Feign Client\n实际操作 微服务B现有的Feign接口如下:\n@FeignClient( name = \u0026#34;eip-bpm-runtime\u0026#34;, url = \u0026#34;${eip-bpm-runtime:}\u0026#34;, fallbackFactory = FlowApiFallback.class ) public interface FlowApi { @RequestMapping( value = {\u0026#34;/flow/instance/v1/start\u0026#34;}, method = {RequestMethod.POST}, produces = {\u0026#34;application/json; charset=utf-8\u0026#34;} ) ObjectNode start(@RequestBody JSONObject var1) throws Exception; } 现在需要在A服务中手动创建一个FlowApi的实例发起调用\n首先在A服务中创建一个配置类\nimport com.aliyun.openservices.shade.com.alibaba.fastjson.support.spring.FastJsonJsonView; import com.fasterxml.jackson.databind.ObjectMapper; import com.hotent.api.FlowApi; import feign.Client; import feign.Feign; import org.apache.http.HttpHeaders; import org.springframework.beans.factory.ObjectFactory; import org.springframework.boot.autoconfigure.http.HttpMessageConverters; import org.springframework.cloud.openfeign.support.SpringDecoder; import org.springframework.cloud.openfeign.support.SpringEncoder; import org.springframework.cloud.openfeign.support.SpringMvcContract; import org.springframework.http.converter.HttpMessageConverter; import org.springframework.http.converter.json.MappingJackson2HttpMessageConverter; import org.springframework.stereotype.Component; import javax.annotation.Resource; /** * BPM 异步feign请求头设置 * * @author kiki * @date 2022/1/13 14:25 * @since 0.0.1 */ @Component public class BpmFlowApiConfig { @Resource private Client client; /** * 接口请求path */ private String url; public void setUrl(String url) { this.url = url; } public FlowApi flowApiClient(String accessToken){ HttpMessageConverter jsonConverter = new MappingJackson2HttpMessageConverter(new ObjectMapper()); ObjectFactory\u0026lt;HttpMessageConverters\u0026gt; converter = () -\u0026gt; new HttpMessageConverters(jsonConverter); return Feign.builder().client(client) .encoder(new SpringEncoder(converter)) .decoder(new SpringDecoder(converter)) .contract(new SpringMvcContract()) .requestInterceptor(template -\u0026gt; template.header(org.springframework.http.HttpHeaders.AUTHORIZATION, accessToken)) .requestInterceptor(template -\u0026gt; template.header(HttpHeaders.CONTENT_TYPE, FastJsonJsonView.DEFAULT_CONTENT_TYPE)) .target(FlowApi.class, url); } } 然后在需要发起对B服务调用的地方注入该配置类,并手动构建Feign客户端发起调用\n/** * desc * * @author kiki * @date 2022/1/13 11:37 * @since 0.0.1 */ @Service @Slf4j public class IAsyncServiceImpl implements IAsyncService { /** * 目标微服务的服务名称 * */ @Value(\u0026#34;${bpm.url:http://eip-bpm-runtime/}\u0026#34;) private String flowApiUrl; @Resource private BpmFlowApiConfig bpmFlowApiConfig; /** * 异步上传发票并发起审批 * * @param paymentAddEo 请求参数 * @param header 上游请求的Token */ @Async @Override public void uploadInvoicesAndStartBpmProcess(PaymentBillAddReqDto paymentAddEo, String header) { // TODO A服务的自身业务逻辑... startNewBpmProcess(paymentAddEo, fileIdList, header); // TODO A服务的自身后续业务逻辑... } private void startNewBpmProcess(PaymentBillAddReqDto paymentAddEo, List\u0026lt;String\u0026gt; fileIdList, String header) { try { // ...省略业务逻辑 log.info(\u0026#34;{}发起bpm申请流程参数：{}\u0026#34;, paymentBillDesc, paramStr); // 这里是发起调用配置的关键 // 1.设置调用服务的服务名 // 2.手动为Feign Client设置header(token) bpmFlowApiConfig.setUrl(flowApiUrl); start = bpmFlowApiConfig.flowApiClient(header).start(JSON.parseObject(paramStr)); ObjectMapper objectMapper = new ObjectMapper(); String resultStr = objectMapper.writeValueAsString(start); log.info(\u0026#34;{}发起BPM审批请求结果：{}\u0026#34;, paymentBillDesc, resultStr); // ...省略业务逻辑 bpmProcessService.add(reqDto); } catch (Exception e) { log.error(\u0026#34;{}失败，bpm返回信息{}，保存的流程信息{}，错误信息：{}\u0026#34;, paymentBillDesc, JSON.toJSONString(start), JSONObject.toJSONString(paymentAddEo, SerializerFeature.PrettyFormat), e); throw new BizException(String.format(\u0026#34;%s发起BPM审批失败！\u0026#34;, e.getMessage())); } } } 其中,A服务在请求发起时,传递的Token需要在A服务发起调用前事先获取\nServletRequestAttributes requestAttributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes(); HttpServletRequest request = requestAttributes.getRequest(); String accessToken = request.getHeader(\u0026#34;Access-Token\u0026#34;); ","permalink":"https://www.atomicbot.cloud/posts/tech/springcloudfeign%E5%9C%A8%E5%BC%82%E6%AD%A5%E6%8E%A5%E5%8F%A3%E6%89%A7%E8%A1%8C%E7%9A%84%E8%BF%87%E7%A8%8B%E4%B8%AD%E4%BC%A0%E9%80%92%E8%AF%B7%E6%B1%82%E5%A4%B4token/","summary":"Spring Cloud Feign 在异步接口执行的过程中传递请求头Token 业务流程 用户发起请求后,在A服务中执行业务逻辑,执行过程中需要调用其他微服务B进行流程审批,简单方式是在A服务中所有逻辑(1,2,3)采用同步逻辑放在一个事务中,但是如果调用B服务的过程中,处理逻辑很复杂,会导致A服务的请求一直阻塞","title":"Spring Cloud Feign在异步接口执行的过程中传递请求头Token"},{"content":" 🔖🔖 211非科班程序员,自学编程混口饭吃,坐标成都,在某中大型食品公司任Java开发工程师一枚。\n主要技术栈涉猎有以下: 💥 Java核心 💥 Spring Boot Spring Cloud生态 💥 Spring生态常用且成熟的中间件 🚀 Vue2+Vue3 (学习中,暂时还差点火候) ⚡️ Mysql数据库与Linux服务器 🚀 CI/CD,持续集成 💥 微服务集群+k8s容器生态 🙈 Flutter App 开发\u0026hellip;(学习中) \u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip; 简单先写这些,个人项目后续完善 🎉 🎉 🎉\n","permalink":"https://www.atomicbot.cloud/about/","summary":"🔖🔖 211非科班程序员,自学编程混口饭吃,坐标成都,在某中大型食品公司任Java开发工程师一枚。 主要技术栈涉猎有以下: 💥 Java核心 💥 Spring Boot Spring Cloud生态 💥 Spring生态常用且成熟的中间件 🚀 Vue2+Vue3 (学习中,暂时还差点火候) ⚡️ Mysql数据库与Linux服务器 🚀 CI/CD,持续集成","title":"我的一些信息🎉🎉🎉 "},{"content":"背景 Redis的主从、哨兵这2中高可用模式已经解决了Redis实例发生故障时的自动主备切换,保证了服务的高可用\n但是这只是解决了服务的可靠性,随着业务不断增长,并发量提升,原有的主从架构已经无法满足性能要求了,这时会出现一些问题:\n单机的物理性能达到极限,不能无限承受流量增加 超额请求、大规模数据计算导致请求响应缓慢 Cluster模式介绍 Cluster即集群模式,类似Mysql,Redis集群也是一种分布式数据库方案,集群通过分片模式来对数据进行管理,并且具备了分片间数据复制、故障转移和流量调度的能力.\nRedis集群的做法是:将数据划分为16384(2的14次方)和哈希槽(slots),如果有多个实例节点,那么每个实例节点管理其中一部分的槽位,槽位的信息回存储在各自归属的节点中,每个节点负责集群中的一部分数据,数据量可以不均匀,比如性能好的节点可以多分摊一些压力\nRedis集群有16384个哈希槽,我们可以使用1-n个节点来分配这些哈希槽,并且可以不均匀分配,每个节点最多处理0-16384个哈希槽,当全部16384个哈希槽都有对应的节点进行管理时,集群处于online状态,如果有任一一个哈希槽没有被管理到,那么集群处于offline状态\n集群节点之间通过gossip协议进行交互,这样每个节点除了有自身哈希槽的分配情况了,也能知道其他节点的哈希槽分配情况\n为什么需要Cluster模式 当单机的吞吐量无法承受持续增加的流量时,最好的办法是横向和纵向进行扩展\n纵向扩展: 提升实例的硬件资源,例如CPU、内存、SSD 横向扩展:横向增加Redis的实例数量,这样便可以降低每个节点负责的哈希槽数量,典型的分治思维 纵向和横向扩展的优缺点:\n纵向扩展比较简单,但是单机物理性能不能无限扩展,单机也无法解决一些瓶颈问题,例如大数据量的持久化(RDB和AOF) 横向扩展也比较容易,但是横向分片会带来更多的问题,新增节点后,哈希槽如何重新分配,节点间的数据如何重新分配 Cluster实现原理 集群组成过程 集群是由一个个独立的Redis节点所组成的,所以刚开始的时候,它们之间是没有任何关系的,各个节点之间通过 CLUSTER MEET \u0026lt;ip\u0026gt; \u0026lt;port\u0026gt;命令完成集群组建,具体做法是:其中一个节点向另外一个节点发送CLUSTER MEET命令,这样2个节点就可以进行握手,握手成功之后,节点就会将另一侧握手的节点添加到当前节点所在的集群中\n集群数据分片原理 现在主流的Redis集群分片做法主要是使用官方的Redis集群方案.这种方案的核心就是集群节点与哈希槽之间的划分、管理与映射\n哈希槽的划分 前面说过,整个Redis集群会划分16384个slots,集群中的所有节点瓜分这些哈希槽,而你具体存储的某个键值信息必然也属于这16384个slots中的某一个,Redis Key与slots的映射步骤如下:\n使用CRC16算法计算键值对的key,得到一个16bit的值 将这个16bit的值对16384取模,得到就是具体的哈希槽的位置,当然一些特殊情况下,也可以将某些key固定到某个slots上,也就是同一个节点上,这时需要使用hash tag能力,强制某个key所归属的slots槽位,使用方式是在key上面加一个{} 127.0.0.1:6380\u0026gt; cluster keyslot user:case{1} (integer) 1024 127.0.0.1:6380\u0026gt; cluster keyslot user:favor (integer) 1023 127.0.0.1:6380\u0026gt; cluster keyslot user:info{1} (integer) 1024 实例中,所有使用了hash tag的键值对都被存放到了同一个哈希槽中\n哈希槽的映射 首先是集群初始化的时候均匀分配,使用CLUSTER CREATE,会将16384个slots平均分配到所有实例上.另一种方式是使用CLUSTER MEET命令,将所有节点联通成一个集群,刚联通的时候由于还没有分配哈希槽,此时集群处于offline状态,可以使用cluster addslots来分配哈希槽.\n比如实例1管理0-7120哈希槽,实例2管理7121-9945哈希槽,实例3管理9946-13005哈希槽,实例4管理13006-16383哈希槽\nredis-cli -h 192.168.0.1 –p 6379 cluster addslots 0,7120 redis-cli -h 192.168.0.2 –p 6379 cluster addslots 7121,9945 redis-cli -h 192.168.0.3 –p 6379 cluster addslots 9946,13005 redis-cli -h 192.168.0.4 –p 6379 cluster addslots 13006,16383 数据复制过程和故障转移 数据复制 cluster是具备master和slave的,每个实例节点负责一部分槽位,每个master至少需要一个slave节点,slave通过主从架构模式同步主节点数据,节点之间保持TCP通信,当master发生宕机时,Redis cluster会自动将对应的slave节点提升为master来继续提供服务,与纯主从模式不同的是,主从节点之间没有读写分离,slave只是作为master的高可用备份节点.\n如果主节点没有从节点,那么master一旦发生故障,集群就会处于完全不可用状态,但也允许配置cluster-require-full-coverage参数,即使部分节点不可用,其他节点也能正常提供服务,主从切换之后,故障恢复的主节点,会转化为新的从节点\n故障检测 一个节点认为某个节点宕机不能说明该节点真的不可用了,只有占据多数的实例认为某个节点挂了,这个时候,集群才进行下线和主从切换的工作\nRedis集群的节点采用gossip协议来广播信息,每个节点会定期向其他节点发送ping命令,如果被ping的节点没有在指定时间内恢复pong,则认为该节点失联,发送ping命令的节点将对方标记为主观下线,如果半数以上的节点都将某个节点标记为主观下线,则这个节点会被标记为客观下线,然后向整个集群广播,并对下线的节点进行主从切换\n主从故障转移 当某个从节点发现自己slaveof的主节点下线了,则开始对下线的主节点进行故障转移,步骤如下:\n如果只有一个slave,那么该节点会执行slaveof no one,成为新的主节点 如果存在多个slave,则采用选举模式竞选出新的master 集群中设立一个自增的计数器,初始值为0,每次执行故障转移选举,计时器+1 检测到主节点下线的从节点向集群的所有master广播一条CLUSTERMSG_TYPE_FAILOVER_AUTH_REQUEST消息,所有收到消息、并具备投票权的主节点都向这个从节点投票 如果收到消息的主节点投票给了当前从节点,则返回CLUSTERMSG_TYPE_FAILOVER_AUTH_ACK表示支持 参与选举的所有从节点都会收到CLUSTERMSG_TYPE_FAILOVER_AUTH_ACK消息,如果某个从节点的收到的选票大于等于半数master,那么这个从节点就被选举为新的master 如果这一轮没有产生新的master,那么计数器+1,再发起一轮投票,直到选出新的master 新的master会撤销所有对已下线master的slots指派,并将这些slots指派给自己 新的master向集群广播一条pong消息,这条消息可以让集群中的其他节点知道这个节点已经由从节点变成了主节点,并且接管了原有master的所有slots 至此,故障转移完成 Client访问集群数据的过程 客户端连接到任意一个实例,获取到所有节点与slots的映射关系,并将映射关系缓存到本地 将需要访问的key经过CRC16计算后,再对16384取模,得到对应的slots索引 根据slots定位到具体的实例,然后将请求发送到对应实例上 ","permalink":"https://www.atomicbot.cloud/posts/tech/redis%E9%AB%98%E5%8F%AF%E7%94%A8%E4%B9%8B%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/","summary":"背景 Redis的主从、哨兵这2中高可用模式已经解决了Redis实例发生故障时的自动主备切换,保证了服务的高可用 但是这只是解决了服务的可靠性,随着业务不断增长,并发量提升,原有的主从架构已经无法满足性能要求了,这时会出现一些问题: 单机的物理性能达到极限,不能无限承受流量增加 超额请求","title":"Redis高可用之集群模式"},{"content":"背景 Redis主从模式由于是读写分离的模式,可以大幅提高性能和服务的可用性,减少甚至避免Redis服务发生宕机的可能,主从模式的主要能力如下:\n故障隔离和恢复: 无论主节点或者从节点崩溃,其他节点仍然可以正常提供服务(主节点崩溃时,无法提供写服务,需要手动切换主从) 读写分离:主节点负责写操作,从节点按照负载均衡的方式提供读服务,分摊了流量和请求压力(极端特殊情况也可以直接从主库读取) 高可用保证: 主从模式是哨兵模式、集群模式的前置条件 主从模式似乎已经挺好了,但是还存在不少问题,从服务故障到手动切换恢复,这可能需要一个比较长的时间,所以我们还需要在主从的基础上为Redis提供感知故障,在master故障时,将某一个可用的slave自动切换为master,实现故障自动转义\n哨兵模式 哨兵模式的核心是主从模式的演变,只不过对于主从模式的主节点崩溃引起无法写入的情况,增加了探活机制:从所有从节点竞选出新的主节点,然后自动奇幻,竞选机制的实现依赖于系统中启动的sentinel进程对各个主从节点的监控\n哨兵的职责 想让Redis实现服务故障自动切换需要考虑的细节有很多:\n判断节点故障的条件?节点可能是假死或者响应延迟 竞选机制,选择那个Slave成为Master 选出新的Master之后,其他Slave需要重新slaveof新的Master,消息是如何通知和通信的 官方对哨兵的定义:\n哨兵作为一种Redis的运行模式,专注于Redis的主从实例进行状态监控,并且能够在主节点发生故障时通过一系列的操作,实现新的Master竞选、主从切换、故障转移,确保整个Redis服务的可用性\n监控 哨兵模式的启用,会同步开启一个叫做sentinel的进程,sentinel进程会向所有的Master和Slave以及其他sentinel进程发送心跳(1s一次),看看是否正常返回响应\n如果Slave没有在规定时间内响应sentinel的ping命令,sentinel会认为该实例不可用,将其标记为:下线状态 同理如果master在规定时间没有响应sentinel的ping命令,也会被判定为下线状态,只不过会多做一个操作,自动进行master的切换流程 ping命令的回复可能有2中情况\n有效回复: 返回 pong -loading -masterdown的任意一种 无效回复: 除有效回复外的情况或者超时没有任何回复 但是sentinel可能存在误判的情况,比如网络抖动、master假死、网络延迟导致某个Redis实例短暂不可用,后续又快速恢复了,如果这个情况下,我们将其主动下线,其实是将整个Redis服务的可用性降低了,所以不能产生这种误判.为了保证判断的可靠性,sentinel将实例标记为下线有2种下线:\n主观下线:哨兵使用 ping监测master和slave,如果是无效回复,哨兵将其标记为主观下线,如果是slave,直接将其下线即可,但如果是master就要小心了,一个sentinel可能存在误判,那么就采用所有哨兵实例一起来判断,就可以避免某个哨兵自身网络情况不好,导致误判将msater下线,同时由所有sentinel一起做决策,误判率也能降低\n客观下线:master是否下线不是某一个sentinel能够决定的,需要集群内所有sentinel一起投票,超过半数的sentinel实例都判断了master主观下线,这个时候才能将master标记为客观下线,认为master真的挂了,当master被标记为客观下线,立即就会启动新master竞选的流程\n如何区别主、客观下线\n主观下线是sentinel自己认为某个节点offline,这时该节点并没有从Redis服务中移除;客观下线是达到一定数量(通常为超过半数)的哨兵都认为该节点offline了,这时汇金一步触发离线、竞选等操作\n这里的超过半数的哨兵是一个法定数量 Quorum,在sentinel.conf中进行配置,该配置主要告诉哨兵需要监听的主节点信息:\n# sentinel monitor \u0026lt;master-name\u0026gt; \u0026lt;master-host\u0026gt; \u0026lt;master-port\u0026gt; \u0026lt;quorum\u0026gt; # 举例如下： sentinel monitor mymaster 127.0.0.1 6379 2 sentinel monitor : 表示监控 mymaster: 主节点名称,可以自定义 127.0.0.1 6379:主节点ip与端口 2:法定数量,只有当2个或者超过2个哨兵实例认为主节点不可用时,才会将master标记为客观下线状态,然后进行failover操作 主从动态切换 从众多slave中选举一个新的master会比较严谨,需要通过筛选+总和评估的方式进行选举\n筛选 过滤掉不健康(下线或者没有回复哨兵ping)的从节点 评估这些实例的历史网络连接状况,down-after-milliseconds,如果在一定周期内(比如24小时)这个从库经常断联并且还超过了指定阈值(比如10次),这种从节点不予考虑 这样筛选后留下的就是比较健康的实例了\n综合评估 筛选掉不健康的实例后,就可以对剩余健康的实例进行综合评估\nslave优先级,通过slave-priority配置项判断,优先级高的从节点优先成为master,该配置在redis.conf中进行配置 选择数据偏移量差距最小的,即master_repl_offset和slave_repl_offset的进度差距,其实就是选择同步进度最高的 slave runID,在优先级和同步进度相同的情况下,选用runID最小的,runID最小,说明该实例创建时间越早,运行时间越长,也可理解为先来后到 这几个条件都评估完后,sentinel集群就会选出最合适的slave,将其推举为新的master\n信息通知 在选出新的master之后,后续客户端的所有写操作都会进入这个master,所以需要尽快通知其他slave重新slaveof到新的master上,重新建立runID和slave_repl_offset,来保证正常的数据服务和主从一致性\n哨兵模式实践 哨兵模式搭建(主从+哨兵) 测试主从功能 测试master自动故障恢复 当前主从状态为:\n18080: master\n18081: slave1\n18082: slave2\n将18080的master停机后观察主从情况:\n重新启动原来的master,查看他的主从情况:\n至此:哨兵模式的自动主从故障切换实践完毕\n关于哨兵集群 哨兵集群如何通信 使用redis的pub/sub订阅能力实现哨兵间通信和slave发现,哨兵与master建立通信之后,可利用master实例的发布/订阅机制发布自身的(哨兵)ip与端口\nmaster有一个sentinel:hello的专用通道,用来给哨兵之间发布和订阅消息,哨兵可以通过该通道发布自己的name、ip、port消息,同时订阅其他哨兵发布的name、ip、port消息,这与微服务中的服务注册于发现类似\n哨兵如何与slave连接 sentinel向master发送info命令 master会返回所有slaveof了自己的从节点信息 sentinel根据master返回的slave列表,逐个与slave建立连接,并根据这个连接持续监控 总结 哨兵集群任务 哨兵机制是实现Redis不间断服务高可用的手段之一,是在master宕机后,自动主从切换提供高可用服务的关键支撑\n监控master与slave的运行状态,判断是否客观下线 master客观下线后,选择一个slave将其推举为新的master 通知其他slave和客户端新的master信息 哨兵集群原理 基于Redis的pub/sub机制实现哨兵间的通信 基于INFO命令获取slave列表,帮助哨兵与slave建立连接 通过哨兵的pub/sub,实现客户端和哨兵之间的时间通知 主从切换,并不是随意选择一个哨兵来执行这一系列的动作,而是通过仲裁投票,选出一个sentinel leader,由这个leader来负责主从切换\n","permalink":"https://www.atomicbot.cloud/posts/tech/redis%E9%AB%98%E5%8F%AF%E7%94%A8%E4%B9%8B%E5%93%A8%E5%85%B5%E6%A8%A1%E5%BC%8F/","summary":"背景 Redis主从模式由于是读写分离的模式,可以大幅提高性能和服务的可用性,减少甚至避免Redis服务发生宕机的可能,主从模式的主要能力如下: 故障隔离和恢复: 无论主节点或者从节点崩溃,其他节点仍然可以正常提供服务(主节点崩溃时,无法提供写服务,需要手动切换主从) 读写分离:主节点负","title":"Redis高可用之哨兵模式"},{"content":"主从复制介绍 Redis的RDB和AOF持久化机制,只是解决了Redis服务崩溃后的快速恢复数据,但是并没有减少或者降低Redis服务崩溃的可能性,也就是没有提高Redis服务的高可用性.\n目前Redis实现高可用服务主要有三种方式: 主从模式、哨兵模式、集群模式.首先来说主从模式.\nRedis的主从模式主要是通过复制的方法,将主节点上的Redis内存数据同步复制一份到从节点上,和Mysql的主从模式很相似,主节点通常称之为master节点,从节点称之为slave节点,主从复制的方向为单向,数据只能从主节点复制到从节点,不能反过来\n主从保证数据一致性 为了保证主节点Redis服务和从节点的Redis服务的数据一致性,同时也为了分担访问压力,负载均衡,通常会在应用程序中采取读写分离的模式对于读操作,可以同时在主节点和从节点执行,因为读取操作是幂等的,通常读取会在从节点上执行,对于实时性和准确性由100%要求的业务,可以谨慎评估后从主节点读取,对于写操作,只能在主节点上执行,主节点执行后将写操作的指令同步到从节点\n为什么采用读写分离 主从模式的读写分离和Mysql做读写分离的动机是一样的.因为已经划分了主库和从库,从库的数据是从主库单向复制来的,如果主库和从库都能执行写操作,那么写入到从库的数据就会在主库中无法查到,导致数据不一致的问题,这是分布式模式的通病.如果非要强行保证数据一致性,不那么Redis就需要加锁,或者使用队列执行,并且由于Redis是单线程的,势必会严重降低性能\n主从模式还有其他什么作用\n故障隔离和恢复: 无论是master崩溃或者是slave崩溃,其他节点依然可以提供读取服务(master崩溃时无法写入),尽量保证程序正常运行,并且可以手动将某个slave重新指定为master继续提供写服务 读写隔离:master负责写入,slave负责读取,分摊流量负载均衡 高可用:主从模式是高可用的基石,哨兵和集群都是基于主从模式而来的 主从模式实践 主从复制的开启,完全是在从节点发起的动作,不需要在主节点上做任何事,通过使用replicaof(5.0版本前使用slaveof)命令可以形成主从关系\n开启主从复制有三种方式:\n配置文件方式 在配置文件中指定master节点的ip和端口 replicaof \u0026lt;masterip\u0026gt; \u0026lt;masterport\u0026gt;\n启动命令方式 在启动Redis实例时加入参数 ./redis-server redis.conf --replicaof 127.0.0.1 18080\n客户端使用命令方式 使用redis-cli登录实例后,手动执行命令,建立主从关系\nSLAVEOF 127.0.0.1 18080\n查看节点间的主从情况 数据同步于读写分离 从实验来看,主节点写入数据后,从节点可以立刻读取到该数据,并且从节点是禁止执行写操作的,数据的写入只能在主节点执行,实现了读写分离\n主从复制原理 主从模式开启后,应用层采用读写分离,所有数据的写操作只会在主库上执行,所有读操作在从库上执行(特殊情况可以从主库读取)\n主从保证数据的最终一致性,主库数据更新后会立即同步到从库\n主从库的同步步骤 主从库的数据同步主要有三种情况\n首次配置主从库时的全量数据同步 主从库正常运行期间的数据实时同步 主从实例网络断开一段时间后重连时对增量数据的同步和实时数据的同步 第一次全量同步 第一次全量同步主要分为三个阶段: 准备阶段 主库同步数据到从库 发送同步期间的增量指令到从库\n建立连接:\n从节点在配置文件中配置了replicaof \u0026lt;masterip\u0026gt; \u0026lt;masterport\u0026gt;参数,从节点就知道需要去哪个主节点同步数据 连接成功后,从库开启replicaof操作,同时发送psync指令告诉主库,我准备开始同步数据了,psync指令包含runId和offset2个参数 runId: 每个redis实例启动成功后会自动生成一个唯一标识,从库第一次同步还不知道主库的runID,所以参数默认时? 第一次复制时,没有偏移量,offset设置为-1,这样就会从主库的第一条指令开始复制 主库收到psync指令后根据参数启动复制,使用FULLRESYNC来响应命令,同时带上2个参数:主库的runID和主库目前的复制进度,返回给从库 从库收到响应后,记录下这2个参数 主库同步数据给从库\nmaster节点执行BGSAVE指令生成RDB文件,将文件发送给从库,从库收到RDB文件后,保存到磁盘,清空当前Redis实例中的数据,再将RDN文件中的数据加载到内存\n主库会为每一个从库开辟一块replication buffer的缓冲区记录,用于记录在RDB文件生成后主库接收到的所有写指令\n发送新的写命令到从库\n从库在初始化完RDB文件中的数据到内存之后,继续执行从replication buffer发送过来的数据,避免出现数据断层\nreplication buffer缓冲区创建在master主库上,存放的是以下时间内master数据的写操作\nmaster执行BGSAVE指令期间的写操作 master传输RDB文件到从库期间的写操作 slave初始化RDB文件到内存期间的写操作 三个步骤完成了Redis从库的第一次全量复制,RDB作为二进制文件,无论是网络传输还是写盘的IO,效率都比AOF更快,所以主从模式会选择RDB文件来做同步\n增量同步 在网络断开之后或者从库故障恢复之后,主从模式会采用增量复制的方式继续同步,而不是全量同步,来提升效率\n从库在故障恢复之后可以实现增量复制的关键就在repl_backlog_buffer缓冲区上面,master会将写指令记录到repl_backlog_buffer中,并使用master_repl_offset记录master写入的位置偏移量,slave则使用slave_repl_offset来读取偏移量,所以master在新增写操作的时候,偏移量会增加,通常情况下,这2个偏移量会保持同步\n但是当网络断开或者从库故障恢复后,主库继续收到写操作,从库暂停执行,就会导致master_repl_offset大于slave_repl_offset\n在从库重新连接之后,从库向主库发送psync指令,并将自己的runID和slave_repl_offset2个参数发给主库,主库只需要将master_repl_offset与slave_repl_offset之间的写操作指令发送给从库即可\n在配置repl_backlog_buffer_size的时候,需要考虑多方因素,太大了会导致增量同步的执行时间较长,还不如RDB全覆盖,太小了的话,从库有可能还没有读取到缓冲区的数据就被master的新的写操作给覆盖了\n一般情况下的缓冲区计算公式: repl_backlog_buffer_size = seconds * write_size_per_second * 1.5\nseconds:正常情况下,主库断开到重连的平均时间,单位秒\nwrite_size_per_second: 主库平均每秒产生的写指令数量\n1.5: 预留容量\n基于长连接的命令传播 以上的同步都是为了完成完成的复制,在完整复制完,主从正常工作开始后进行同步需要保持长连接,在主库接收到写操作指令后,通过这个长连接同步给从库,这个过程称之为基于长连接的命令传播,为了宝成传播的有效性和稳定性,从节点采用心跳机制进行侦测,发送命令 PING和REPLCONF ACK\n每隔一定时间(配置中配置),比如30S,主节点会向从节点发送PING命令来判断从节点的健康情况.\n命令传播阶段,从节点会默认以每秒的频率向主服务器发送命令,将从节点已经复制的偏移量发送过去 REPLCONF ACK \u0026lt;replication_offset\u0026gt;, replication_offset属性是指当前从节点已经从主节点复制的偏移量,从节点发送这个REPLCONF ACK命令的主要作用有3个:\n检查主节点的健康状况和网络情况 辅助实现min-slaves选项,Redis的min-slaves-to-write(少于N个从节点时拒绝执行写命令)和min-slaves-lag(主从延迟大于等于N秒时,拒绝执行写命令),这2个选项可以防止主服务器在不安全的情况下执行写命令 检测数据丢失: 从节点发送了slave_repl_offset偏移量,主节点会和master_repl_offset进行对比,如果不一致,说明从节点丢失了数据,主节点会从repl_backlog_buffer中将2个offset区间的指令找到并推送给从节点 如何确定全量还是增量 核心点就是从节点发送的psync指令:\n从节点根据自身是否需要全量发送psync给主节点 如果从未执行过replicaof,则发送psync ? -1,代表全量复制 如果之前同步过,则取当前实例的runID和已同步的offset发送指令 psync \u0026lt;runID\u0026gt; \u0026lt;slave_repl_offset\u0026gt; 主节点根据psync质量的参数决定是全量同步还是增量同步 主,从节点的runID一致,并且从节点发送的slave_repl_offset在repl_backlog_buffer中存在(环形闭合的内存缓冲区,有可能被擦除了),则回复CONTINUE,代表以增量模式进行复制 主,从节点的runID不一致,或者从节点发送的slave_repl_offset在repl_backlog_buffer中不存在(可能是从节点宕机时间过久),则回复FULLRESYNC \u0026lt;runId\u0026gt; \u0026lt;offset\u0026gt;表示要进行全量复制,同时记录下主节点的runID和offset 一主多从同步的理解 从以上内容可以得知:\n多个从库的情况下,每个从库都会记录自己的slave_repl_offset,各自的复制进度也不相同 从库重连进行恢复时,会通过psync命令将自己的slave_repl_offset告诉主库,主库来判断是增量复制还是全量复制 replication buffer和repl_backlog_buffer的说明:\nreplication buffer是主从库在进行全量复制时,主库上用于和从库连接的客户端buffer,repl_backlog_buffer是为了支持从库的增量复制,主库上用于持续记录主库写操作的一块专用buffer,所有从库共享.\n","permalink":"https://www.atomicbot.cloud/posts/tech/redis%E9%AB%98%E5%8F%AF%E7%94%A8%E4%B9%8B%E4%B8%BB%E4%BB%8E%E6%A8%A1%E5%BC%8F/","summary":"主从复制介绍 Redis的RDB和AOF持久化机制,只是解决了Redis服务崩溃后的快速恢复数据,但是并没有减少或者降低Redis服务崩溃的可能性,也就是没有提高Redis服务的高可用性. 目前Redis实现高可用服务主要有三种方式: 主从模式、哨兵模式、集群模式.首先来说主从模式. R","title":"Redis高可用之主从模式"},{"content":"介绍 ","permalink":"https://www.atomicbot.cloud/posts/tech/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E4%B9%8B-%E4%B8%80%E8%87%B4%E6%80%A7hash%E7%AE%97%E6%B3%95/","summary":"介绍","title":"分布式算法之--一致性hash算法.md"},{"content":"介绍 Redis通常用来作为应用程序与数据库DB之间的一层缓存中间件,来保证数据的访问效率,那既然是一层中间件,必然存在宕机、崩溃的问题,一旦Redis服务不可用,可能产生的后果包括:\nRedis进程中保存的数据全部丢失,引文Redis数据是存在RAM中的 数据访问从内存级别降低至IO级别,性能下降明显 原本大量访问缓存的请求现在都请求到数据库,可能导致数据库压力过大,甚至雪崩 所以Redis中间件崩溃后的结果是很严重的,为了避免宕机后RAM中的数据丢失,Redis提供2中数据持久化的能力,RDB (Redis Database)和AOF (Append Only File)\n关于RDB Redis和Mysql的最大区别就是一个将数据存储在内存,一个存储在磁盘,如果每次数据的变化CUD都要同时写内存和硬盘,那性能成本就太高了,而且还需要保证原子性,避免内存和磁盘数据的一致性\nRDB 为了避免修改数据时实时写入内存,Redis提供了RDB,内存快照功能,内存快照是指在某一时刻,Redis内存中数据的状态,快照文件就称之为RDB文件,Redis可以通过定期执行RDB内存快照,将Redis内存中的数据备份到磁盘,便可以避免高频的更新内存,既保证了Redis的高效读写,还实现了宕机后快速恢复数据\nRDB快照 Redis提供2种方式生成RDB快照文件:\nSAVE: 由主线程来执行,同步阻塞,只有当save命令保存完快照后,主线程才能响应应用程序的请求 BGSAVE:执行后,会立即返回OK,同时调用glibc的函数,fork出一个子进程来持久化快照,快照的持久化完全交给子进程,主进程对于应用程序的读写不阻塞,相当于写RDB的操作是一个异步动作 SAVE模式 SAVE模式下的快照持久化操作是主进程在执行,由于Redis是单线程,会阻塞应用程序的请求,不建议使用\nBGSAVE模式 Redis使用操作系统的多进程写时复制COW(copy on write)来实现快照持久化,由于fork出来的子进程和Redis主进程共享内存,所以子进程读取主进程的数据来写入RDB文件,主进程可以继续响应应用程序的请求,2者互不影响,并且在创建RDB文件时,程序会对内存中的数据做一遍检查,仅仅会将未过期的键值保存到RDB文件中\nRDB配置 RDB模式何时进行快照RDB文件的存储,是在Redis配置文件中进行指定,以下截取部分源码中的配置说明\n避免RDB执行频率过高 虽然说RDB快照存储,是在子进程中进行的,不会影响主进程对客户端的请求处理,但是如果RDB执行频率过高就会导致严重的性能开销\n频繁的创建RDB文件写入内存,导致磁盘压力过大,效率降低 fork出来的子进程由于共享主线程的数据,一定程度上也会阻塞主线程的运行,主进程使用的内存越大,存储的数据越多,阻塞时间越长 RDB总结 快照RDB是对内存数据进行的全量持久化,但是生成RDB需要把握一个度,频率太快会导致额外开销过大,性能降低,频率太低的话,RDB文件和内存中的数据差异就越大,丢失的数据就越多 RDB建议采用二进制+数据压缩方式写入磁盘,默认也是开启RDB压缩的 AFO日志文件 AOF日志文件存储的是Redis对内存的修改的指令记录,假设AOF日志文件记录了自Redis实例创建以来的所有修改内存的指令序列,那么可以对一个空的Redis实例按照顺序执行这些指令就可以还原所有数据,类似与git的rebase\n日志变更对比 AOF记录日志有2种方式:\n写前日志:在执行Redis命令之前,先将指令记录到AOF日志文件,然后再执行指令,类似于MySql种的redo log,修改数据前先记录日志 写后日志:先执行指令,然后再将指令写入AOF日志文件 日志格式 Redis在接收到set keyName someValue指令的时候,会先将数据写到内存,然后按照如下的格式写入 AOF文件中. *3表示该条指令包含三个部分,每个部分都是$ + 数字开头,后面是每个部分的具体内容: 指令、键、值,数字表示这部分命令占用的自节大小\n推荐使用写后日志的模式,因为这样在记录日志时不用对指令做语法检查,如果使用写前日志,需要对指令做语法检查\n可能存在的问题 可能存在数据丢失:Redis执行完指令后还没有写入AOF日志成功就宕机了 AOF避免了当前明亮的阻塞,但是AOF是主线程在执行,将日志写入磁盘的过程中,如果IO压力过大,就会导致执行缓慢 写回策略 上面的问题在Redis高频读写时是必然存在的,想要解决,在写入时做一层缓冲就可以了,避免直塞,Redis提供了一种执行策略叫做回写策略\n回写策略说明 为了提高AOF日志文件的写入效率,回写策略会做入下调整:\n在调用write函数项日志文件写入数据时,并不是真正的落盘,而是将数据写入到操作系统的内存缓冲区. 当缓冲区的空间被填满或者超过了指定的阈值,才会真正将缓冲区的数据写入到磁盘中,这种方式虽然提高了效率,但是可能存在数据安全性问题,如果Redis发生了宕机,那么缓冲区的数据就会丢失,为此系统提供了fsync和sdatasync2个同步函数,它们可以强制让操作系统立即将缓冲区中的数据写入到硬盘中 Redis的AOF配置项appendfsync写回策略直接决定了持久化功能的效率和安全性,以下是appendfsync的具体配置\nalways:同步写回,每次执行完指令,就将缓冲区的数据写回到AOF文件中 everysec:每秒写回,执行完命令后,将数据写入到缓冲区,缓冲区每隔一秒将数据写入到AOF文件 no:操作系统控制,在执行完指令后,将数据写入到缓冲区,由操作系统决定何时写回到磁盘 写磁盘会带来性能损耗,这取决于你的应用程序对性能和数据可靠性的取舍\nalways: 性能最差,安全性最好,数据不会丢失 everysec:如果Redis宕机,可能会丢失1秒时间周期的数据,在性能和可靠性上折中 no:性能最好,但有可能丢失更多数据 写回策略的选择 系统要求高性能: 选择no策略 系统要求数据高可靠性:选择always策略 能够接收少量数据丢失又想要较为良好的性能:选择everysec策略(默认策略和推荐策略) 混合RDB和AOF模式 无论是RDB还是AOF总是感觉不够尽善尽美,使用RDB来恢复内存状态,势必会丢失一部分数据,使用AOF日志文件重放对性能又有一定影响,因为重放的AOF日志文件可能会很大,Redis在4.0解决了这个问题,采用一种新的持久化方式:混合持久化 该模式默认是关闭的\n将RDB文件和生成RDB快照时间点后的AOF增量日志存放在一起,这个时候AOF文件中就不再是整个实例的全量日志,而是最近一次RDB快照点之后发生的增量日志,通常这部分AOF文件会很小,所以执行顺序就变成如下:\n查找RDB内容,如果存在就先加载RDB文件,然后再重放AOF日志\n没有RDB文件,就直接重放AOF文件\n这样快照就不用频繁的执行,同时由于AOF文件记录的是最近的快照点之后的指令,避免了单词重放日志文件过大的问题\n","permalink":"https://www.atomicbot.cloud/posts/tech/redis%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96%E9%AB%98%E5%8F%AF%E7%94%A8/","summary":"介绍 Redis通常用来作为应用程序与数据库DB之间的一层缓存中间件,来保证数据的访问效率,那既然是一层中间件,必然存在宕机、崩溃的问题,一旦Redis服务不可用,可能产生的后果包括: Redis进程中保存的数据全部丢失,引文Redis数据是存在RAM中的 数据访问从内存级别降低至IO","title":"Redis的数据持久化"}]